{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symptom Keyword querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('symptom.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute('SELECT Symptom.Symp_name, Value.Keyword FROM Value, Symptom WHERE Value.Symp_id == Symptom.Symp_id')\n",
    "res = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cur.execute('SELECT Symp_id, Symp_name FROM Symptom')\n",
    "symptom_value = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S0', '견배통'),\n",
       " ('S1', '구토'),\n",
       " ('S2', '열'),\n",
       " ('S3', '오한'),\n",
       " ('S4', '발열'),\n",
       " ('S5', '두통'),\n",
       " ('S6', '이명'),\n",
       " ('S7', '요통'),\n",
       " ('S8', '현훈'),\n",
       " ('S9', '건망'),\n",
       " ('S10', '주관절통증'),\n",
       " ('S11', '손목'),\n",
       " ('S12', '이롱'),\n",
       " ('S13', '번심'),\n",
       " ('S14', '다한'),\n",
       " ('S15', '변비'),\n",
       " ('S16', '피로'),\n",
       " ('S17', '족관절염좌'),\n",
       " ('S18', '손목 무력'),\n",
       " ('S19', '소화장애'),\n",
       " ('S20', '인후염'),\n",
       " ('S21', '염좌'),\n",
       " ('S22', '손바닥'),\n",
       " ('S23', '생리통'),\n",
       " ('S24', '비출혈'),\n",
       " ('S25', '구갈'),\n",
       " ('S26', '천식'),\n",
       " ('S27', '구안와사'),\n",
       " ('S28', '치통'),\n",
       " ('S29', '근육통'),\n",
       " ('S30', '상지통'),\n",
       " ('S31', '오심'),\n",
       " ('S32', '수관절장애'),\n",
       " ('S33', '기침'),\n",
       " ('S34', '복통'),\n",
       " ('S35', '불면'),\n",
       " ('S36', '소변불통'),\n",
       " ('S37', '설사'),\n",
       " ('S38', '중풍'),\n",
       " ('S39', '흉통'),\n",
       " ('S40', '두드러기'),\n",
       " ('S41', '무력'),\n",
       " ('S42', '수족냉증'),\n",
       " ('S43', '비염'),\n",
       " ('S44', '열사병'),\n",
       " ('S45', '황달'),\n",
       " ('S46', '안구피로'),\n",
       " ('S47', '산후무유'),\n",
       " ('S48', '손바닥열'),\n",
       " ('S49', '실신'),\n",
       " ('S50', '코감기'),\n",
       " ('S51', '심병'),\n",
       " ('S52', '백내장'),\n",
       " ('S53', '눈병'),\n",
       " ('S54', '감기'),\n",
       " ('S55', '구내염'),\n",
       " ('S56', '요실금'),\n",
       " ('S57', '수관절통증'),\n",
       " ('S58', '족관절')]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symptom_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT max(Value_id) FROM Value')\n",
    "max_Value = cur.fetchall()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Symp_id = ?\n",
    "for i in range(int(max_Value[1:]) + 1,):\n",
    "    cur.execute('INSERT INTO Value_id, Symp_id, Keyword VALUES(:V_id, :S_id, :Keyword)', {'V_id' : 'V'+str(i),'S_id': Symp_id, 'Keyword' : Keyword})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('INSERT INTO Value VALUES(:a, :b)', {'a': ,'b': age})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symptom Keywords will be merged as a One Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val = {}\n",
    "for i in range(len(res)):\n",
    "    \n",
    "    key = res[i][0]\n",
    "    value = res[i][1]\n",
    "    if res[i][1][-1] == '다':\n",
    "        value = res[i][1] + '.'\n",
    "    \n",
    "    try:\n",
    "        val[key].append(value)\n",
    "\n",
    "    except KeyError:\n",
    "        val[key] = []\n",
    "        val[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'건망': ['깜박깜박 한다.',\n",
       "  '건망증',\n",
       "  '기억이 나빠진다.',\n",
       "  '기억력 감퇴',\n",
       "  '자꾸 잊는다.',\n",
       "  '까먹는다.',\n",
       "  '쉽게 잊는다.',\n",
       "  '잘 잊어버린다.',\n",
       "  '기억력이 약해지다.',\n",
       "  '기억력 감소'],\n",
       " '견배통': ['어깨가 당긴다.',\n",
       "  '등이 아프다.',\n",
       "  '어깨를 들 수 없다.',\n",
       "  '어깨가 무너지는 듯 하다.',\n",
       "  '목덜미가 뻣뻣하다.',\n",
       "  '목덜미가 아파 돌아볼 수 없다.',\n",
       "  '어깻죽지가 피곤하고 답답하다.',\n",
       "  '등이 뻣뻣하다.',\n",
       "  '등과 어깻죽지가 아프다.',\n",
       "  '목 주위가 뭉치고 답답하다.',\n",
       "  '목을 펼 수 없다.',\n",
       "  '고개를 돌릴 수 없다.',\n",
       "  '뒷목이 뻣뻣하다.',\n",
       "  '목을 굽힐 수 없다.',\n",
       "  '목에 담이 왔다.',\n",
       "  '뒷목이 아프다.',\n",
       "  '목을 젖힐 수 없다.',\n",
       "  '어깨가 당기며 아프다.'],\n",
       " '구갈': ['입이 마르다.',\n",
       "  '입 건조',\n",
       "  '목이 마르다.',\n",
       "  '물이 당긴다.',\n",
       "  '입에 갈증이 난다.',\n",
       "  '물을 찾는다.',\n",
       "  '물을 마시고 싶다.',\n",
       "  '목이 건조하고 마르다.',\n",
       "  '갈증이 난다.'],\n",
       " '눈병': ['입이 헐다.', '입이 아프다.', '혀가 헐다.', '혀가 붓다.', '입속이 붓다.', '혀가 아프다.'],\n",
       " '구토': ['토하다.'],\n",
       " '수관절장애': ['콜록거린다.', '잔기침이 난다.', '마른 기침을 하다.', '기침'],\n",
       " '심병': ['눈이 아프다.',\n",
       "  '눈이 빨개진다.',\n",
       "  '결막염이 생기다.',\n",
       "  '눈이 붉어지다.',\n",
       "  '눈이 아프고 눈물이 난다.',\n",
       "  '눈꺼풀이 붓다.',\n",
       "  '눈이 충혈되다.'],\n",
       " '다한': ['땀이 많이 난다.', '땀으로 축축하다.', '땀이 줄줄 난다.', '땀에 푹 젖는다.', '땀이 흥건하다.'],\n",
       " '흉통': ['피부에 붉게 올라온다.',\n",
       "  '참을 수 없이 가렵다.',\n",
       "  '반진',\n",
       "  '풍진',\n",
       "  '가려움이 멎지 않는다.',\n",
       "  '몸이 가렵다.',\n",
       "  '은진',\n",
       "  '피부가 가렵다.'],\n",
       " '두통': ['머리가 찌르는 것 같다.',\n",
       "  '머리아프다.',\n",
       "  '머리가 극심히 아프다.',\n",
       "  '머리가 찌르듯 아프다.',\n",
       "  '머리가 욱신거리다.',\n",
       "  '머리가 깨질 것 같다.',\n",
       "  '머리가 터질것 같다.',\n",
       "  '머리를 싸맨 듯 하다.',\n",
       "  '머리가 쪼개질 듯 하다.',\n",
       "  '머리가 타는 듯 하다.',\n",
       "  '머리를 침으로 찌르는 듯 하다.',\n",
       "  '머리가 부풀어 깨질 듯 하다.',\n",
       "  '머리가 무겁고 아프다.'],\n",
       " '발열': ['화끈거린다.', '열이 난다.', '열 난다.', '몸이 뜨겁다.'],\n",
       " '코감기': ['눈이 침침하다.', '백내장'],\n",
       " '번심': ['가슴이 답답하다.',\n",
       "  '가슴에 열이 난다.',\n",
       "  '속에 불이 난다.',\n",
       "  '속에 열이 난다.',\n",
       "  '속이 답답하다.',\n",
       "  '가슴의 답답하다.',\n",
       "  '속이 답답하다.',\n",
       "  '안절부절 못하다.',\n",
       "  '번열이 나다.',\n",
       "  '가슴이 불편하다.'],\n",
       " '기침': ['배를 쥐어짜는 듯 하다.', '배가 아프다.'],\n",
       " '복통': ['자다 깨다.',\n",
       "  '잠을 잘 자지 못하다.',\n",
       "  '잠을 조금밖에 자지 못하다.',\n",
       "  '꿈을 많이 꾼다.',\n",
       "  '잠을 못잔다.',\n",
       "  '밤새 뒤척거린다.',\n",
       "  '푹 자지 못한다.'],\n",
       " '비출혈': ['코에 피가 난다.', '비뉵', '코피가 난다.', '코피가 멎지 않는다.', '코피를 흘리다.', '코피가 흐른다.'],\n",
       " '안구피로': ['수유량이 적다.'],\n",
       " '생리통': ['아랫배가 아프다.',\n",
       "  '생리로 통증이 심하다.',\n",
       "  '아랫배 통증을 참을 수 없다.',\n",
       "  '아랫배를 쥐어짜는 듯 하다.',\n",
       "  '아랫배를 비트는 것처럼 아프다.',\n",
       "  '아랫배가 땡긴다.',\n",
       "  '아랫배가 뻐근하다.',\n",
       "  '허리가 뻐근하다.',\n",
       "  '생리통이 심하다.'],\n",
       " '소변불통': ['화장실에 자주 가다.', '설사가 그치지 않다.', '대변이 무르다.', '대변이 묽다.', '물설사를 하다.'],\n",
       " '소화장애': ['배가 빵빵하다.',\n",
       "  '소화가 안된다.',\n",
       "  '속이 더부룩하다.',\n",
       "  '배가 부풀다.',\n",
       "  '속이 부대끼다.',\n",
       "  '소화불량',\n",
       "  '배가 불러오르며 아프다.',\n",
       "  '가스가 찬다.'],\n",
       " '손목 무력': ['손을 짚기 힘들다.', '손목에 힘이 없다.'],\n",
       " '산후무유': ['손바닥이 뜨겁다.', '손바닥에 열이난다.', '손바닥이 화끈거린다.'],\n",
       " '오심': ['요골신경장애', '손목이 아프다.', '척골신경장애'],\n",
       " '무력': ['발이 얼음장같다.', '손이 차다.', '손발이 차다.', '발이 차다.', '손이 얼음장같다.'],\n",
       " '손바닥열': ['쓰러지다.'],\n",
       " '황달': ['눈이 빨개진다.',\n",
       "  '눈이 충혈되다.',\n",
       "  '눈이 건조하다.',\n",
       "  '눈이 빠질 것 같다.',\n",
       "  '눈알이 깔깔하다.',\n",
       "  '눈이 피로하다.'],\n",
       " '비염': ['더위로 어지럽다.', '더위먹다.'],\n",
       " '근육통': ['속이 울렁거린다.',\n",
       "  '메스껍다.',\n",
       "  '구역감',\n",
       "  '구역질이 난다.',\n",
       "  '헛구역질이 난다.',\n",
       "  '토할 것 같다.',\n",
       "  '속이 불쾌하다.',\n",
       "  '오심',\n",
       "  '메슥거리다.'],\n",
       " '백내장': ['상기도감염',\n",
       "  '기관지염',\n",
       "  '으실으실하다.',\n",
       "  '몸이 차다.',\n",
       "  '으슬으슬하다.',\n",
       "  '몸이 떨린다.',\n",
       "  '감기 기운이 있다.',\n",
       "  '감기 걸렸다.',\n",
       "  '온 몸이 춥다.'],\n",
       " '불면': ['소변을 보지 못하다.',\n",
       "  '소변이 시원하지 않다.',\n",
       "  '소변보기 불편하다.',\n",
       "  '소변보기 어렵다.',\n",
       "  '소변이 잘 나오지 않는다.'],\n",
       " '감기': ['소변이 저절로 나온다.',\n",
       "  '소변을 조절하지 못한다.',\n",
       "  '소변을 지린다.',\n",
       "  '소변을 참지 못하다.',\n",
       "  '소변이 새다.',\n",
       "  '소변 실수를 한다.'],\n",
       " '요통': ['허리를 돌리지 못하다.',\n",
       "  '앉을 수 없다.',\n",
       "  '허리를 젖힐 수 없다.',\n",
       "  '요추가 아프다.',\n",
       "  '협착증이 있다.',\n",
       "  '디스크',\n",
       "  '디스크가 터졌다.',\n",
       "  '허리 통증이 심하다.',\n",
       "  '허리가 아프다.',\n",
       "  '허리를 굽힐 수 없다.'],\n",
       " '이롱': ['소리를 잘 듣지 못하다.',\n",
       "  '잘 안들린다.',\n",
       "  '귀가 잘 안들린다.',\n",
       "  '귀가 먹었다.',\n",
       "  '청력이 떨어진다.',\n",
       "  '청력이 감소했다.'],\n",
       " '이명': ['귀에 매미소리가 난다.', '귀에서 이상한 소리가 난다.', '귀에서 삐 하는 소리가 난다.', '이명이 들린다.'],\n",
       " '인후염': ['목구멍에 염증이 발생하다.',\n",
       "  '목구멍에 딴 물질이 들어간 느낌',\n",
       "  '인후부 자극감',\n",
       "  '목구멍이 붓는다.',\n",
       "  '목이 붉게 부어오르다.',\n",
       "  '귀 통증',\n",
       "  '침 삼키기 힘들다.',\n",
       "  '음식 삼키기 어렵다.',\n",
       "  '음식 삼키기 불편하다.',\n",
       "  '목이 아프다.',\n",
       "  '목에 이물감이 있다.',\n",
       "  '목감기',\n",
       "  '인후에 염증이 생겼다.',\n",
       "  '목이 쉬다.',\n",
       "  '목이 부었다.',\n",
       "  '목 따갑다.',\n",
       "  '편도가 부었다.',\n",
       "  '인후염',\n",
       "  '편도염',\n",
       "  '목이 가렵다.'],\n",
       " '족관절염좌': ['발목을 다치다.',\n",
       "  '발을 삐다.',\n",
       "  '발목이 삐었다.',\n",
       "  '발목을 접질렀다.',\n",
       "  '발목 관절이 삐다.',\n",
       "  '발목이 붓다.'],\n",
       " '주관절통증': ['팔꿈치가 아프다.'],\n",
       " '설사': ['편마비',\n",
       "  '다리를 절다.',\n",
       "  '마비',\n",
       "  '사지마비',\n",
       "  '말하기 힘들고 마비가 오다.',\n",
       "  '혀가 굳는다.',\n",
       "  '의식이 흐려지다.',\n",
       "  '풍이 오다.',\n",
       "  '얼굴에 감각이 없다.',\n",
       "  '얼굴이 마비되다.',\n",
       "  '얼굴이 굳다.',\n",
       "  '와사'],\n",
       " '천식': ['콜록거린다.',\n",
       "  '천명음이 난다.',\n",
       "  '호흡곤란',\n",
       "  '가래가 찬다.',\n",
       "  '숨이 찬다.',\n",
       "  '객담이 생긴다.',\n",
       "  '숨이 가쁘다.'],\n",
       " '치통': ['이가 시리다.', '이빨이 아프다.', '이가 아프다.'],\n",
       " '수족냉증': ['코가 난다.', '코막힘', '콧물이 흐른다.', '코가 막히다.', '콧물이 난다.'],\n",
       " '현훈': ['어지럽다.',\n",
       "  '눈앞이 아찔하다.',\n",
       "  '정신이 흐릿하고 혼미하다.',\n",
       "  '눈앞이 캄캄하다.',\n",
       "  '머리가 핑핑 돈다.',\n",
       "  '머리가 띵하다.',\n",
       "  '주위가 빙빙 도는 듯 하다.',\n",
       "  '눈 앞에 헛 것이 보인다.',\n",
       "  '머리가 핑 돌며 어지럽다.',\n",
       "  '헛것이 어른거린다.',\n",
       "  '현기증난다.'],\n",
       " '열사병': ['몸이 누렇게 되다.',\n",
       "  '누렇게 뜨다.',\n",
       "  '눈이 노랗다.',\n",
       "  '얼굴이 노랗다.',\n",
       "  '온몸이 노랗다.',\n",
       "  '손발이 노랗다.'],\n",
       " '중풍': ['가슴이 아프다.',\n",
       "  '가슴이 바늘로 찌르듯 아프다.',\n",
       "  '가슴이 땅긴다.',\n",
       "  '심장이 아프다.',\n",
       "  '가슴부위 불쾌감이 있다.',\n",
       "  '가슴부위 통증이 있다.',\n",
       "  '가슴통증'],\n",
       " '실신': ['눈물이 난다.',\n",
       "  '감정 조절이 안된다.',\n",
       "  '짜증이 난다.',\n",
       "  '스트레스 받다.',\n",
       "  '불안하다.',\n",
       "  '가슴이 두근거린다.',\n",
       "  '우울하다.',\n",
       "  '심장이 불규칙하게 뛴다.',\n",
       "  '심박동이 불규칙하다.',\n",
       "  '부정맥이 있다.'],\n",
       " '피로': ['기력이 없다.',\n",
       "  '무기력하다.',\n",
       "  '움직일 기운이 없다.',\n",
       "  '기운이 없다.',\n",
       "  '허약해지다.',\n",
       "  '과로하다.',\n",
       "  '몸이 허하다.',\n",
       "  '피로가 가시지 않는다.',\n",
       "  '몸이 무겁다.',\n",
       "  '지친다.']}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "pattern1 = re.compile(r'[{}]'.format(re.escape(punctuation)))\n",
    "pattern2 = re.compile(r'\\b(\\w|[.])+@(?:[.]?\\w+)+\\b')\n",
    "pattern3 = re.compile(r'\\bhttps?://\\w+(?:[.]?\\w+)+\\b')\n",
    "pattern4 = re.compile(r'[^A-Za-z0-9가-힣ㄱ-ㅎㅏ-ㅣ ]')\n",
    "pattern5 = re.compile(r'\\b[a-z][A-Za-z0-9]+\\b')\n",
    "pattern6 = re.compile(r'\\s{2,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2. Indexer\n",
    "from os import listdir\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def fileids(path = './symp/'):\n",
    "    return [path+_ for _ in listdir(path)\n",
    "            if re.search('[.]txt$', _)]\n",
    "\n",
    "def cleaning(doc):\n",
    "    return pattern6.sub(' ',\n",
    "           pattern1.sub(' ',\n",
    "           pattern5.sub(' ',\n",
    "           pattern4.sub(' ',\n",
    "           pattern2.sub(' ', doc))))).strip()\n",
    "\n",
    "def tokenizer1(doc): # 어절\n",
    "    return doc.split()\n",
    "\n",
    "def tokenizer2(doc, n=2): # 어절 Ngram\n",
    "    tmp = doc.split()\n",
    "    ngram = list()\n",
    "    for i in range(0, len(tmp) - n + 1):\n",
    "        token = ''\n",
    "        for j in range(i, i + n):\n",
    "            token += tmp[j] + ' '\n",
    "        ngram.append(token)\n",
    "    return ngram\n",
    "\n",
    "def tokenizer3(doc, n=2): # 음절 Ngram\n",
    "    ngram = list()\n",
    "    for i in range(len(doc) - (n-1)):\n",
    "        ngram.append(doc[i:i+n])\n",
    "    return ngram\n",
    "\n",
    "def tokenizer4(doc): # 형태소\n",
    "    return [_ for _ in okt.morphs(doc) if 1 < len(_) < 8]\n",
    "\n",
    "def tokenizer5(doc): # 명사\n",
    "    return [_ for _ in okt.nouns(doc) if 1 < len(_) < 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# each symptom txt will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "symp_list = list(val)\n",
    "for i in range(len(symp_list)):\n",
    "    \n",
    "    symp = symp_list[i]\n",
    "    a = ''\n",
    "\n",
    "    for i in range(len(val[symp])):\n",
    "        a += val[symp][i] + ' '\n",
    "               \n",
    "    with open('./symp/' + symp + '.txt', mode = 'w') as f:\n",
    "        f.write(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wposting, FILES, globalDictionary update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((9, 0.5421944699333662), '머리가 찌르는 것 같다. 머리아프다. 머리가 극심히 아프다. 머리가 찌르듯 아프다. 머리가 욱신거리다. 머리가 깨질 것 같다. 머리가 터질것 같다. 머리를 싸맨 듯 하다. 머리가 쪼개질 듯 하다. 머리가 타는 듯 하다. 머리를 침으로 찌르는 듯 하다. 머리가 부풀어 깨질 듯 하다. 머리가 무겁고 아프다.')\n",
      "((5, 0.4279604500510067), '속이 울렁거린다. 메스껍다. 구역감 구역질이 난다. 헛구역질이 난다. 토할 것 같다. 속이 불쾌하다. 오심 메슥거리다.')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "import struct\n",
    "from struct import pack, unpack\n",
    "from os import listdir\n",
    "from konlpy.tag import Okt\n",
    "from math import sqrt\n",
    "from math import log\n",
    "import json\n",
    "\n",
    "# 증상 검색\n",
    "class Search_symptom:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern1 = re.compile(r'[{}]'.format(re.escape(punctuation)))\n",
    "        self.pattern2 = re.compile(r'\\b(\\w|[.])+@(?:[.]?\\w+)+\\b')\n",
    "        self.pattern3 = re.compile(r'\\bhttps?://\\w+(?:[.]?\\w+)+\\b')\n",
    "        self.pattern4 = re.compile(r'[^A-Za-z0-9가-힣ㄱ-ㅎㅏ-ㅣ ]')\n",
    "        self.pattern5 = re.compile(r'\\b[a-z][A-Za-z0-9]+\\b')\n",
    "        self.pattern6 = re.compile(r'\\s{2,}')\n",
    "\n",
    "        self.okt = Okt()\n",
    "\n",
    "    # 증상 정렬 파일 탐색\n",
    "    def fileids(self, path = './symp/'):\n",
    "\n",
    "        return [path+_ for _ in listdir(path) if re.search('[.]txt$', _)]\n",
    "\n",
    "    def cleaning(self, doc): # 전처리\n",
    "\n",
    "        return self.pattern6.sub(' ',\n",
    "               self.pattern1.sub(' ',\n",
    "               self.pattern5.sub(' ',\n",
    "               self.pattern4.sub(' ',\n",
    "               self.pattern2.sub(' ', doc))))).strip()\n",
    "\n",
    "    def tokenizer1(self, doc): # 어절\n",
    "\n",
    "        return doc.split()\n",
    "\n",
    "    def tokenizer2(self, doc, n=2): # 어절 Ngram\n",
    "        tmp = doc.split()\n",
    "        ngram = list()\n",
    "        for i in range(0, len(tmp) - n + 1):\n",
    "            token = ''\n",
    "            for j in range(i, i + n):\n",
    "                token += tmp[j] + ' '\n",
    "            ngram.append(token)\n",
    "        return ngram\n",
    "\n",
    "    def tokenizer3(self, doc, n=2): # 음절 Ngram\n",
    "\n",
    "        ngram = list()\n",
    "        for i in range(len(doc) - (n-1)):\n",
    "            ngram.append(doc[i:i+n])\n",
    "        return ngram\n",
    "\n",
    "    def tokenizer4(self, doc): # 형태소\n",
    "\n",
    "        return [_ for _ in self.okt.morphs(doc) if 1 < len(_) < 8]\n",
    "\n",
    "    def tokenizer5(self, doc): # 명사\n",
    "\n",
    "        return [_ for _ in self.okt.nouns(doc) if 1 < len(_) < 8]\n",
    "\n",
    "    def get_tokens(self, file): # 단어를 작은 단위로 분리\n",
    "\n",
    "        terms = defaultdict(lambda:0)\n",
    "\n",
    "        with open(file, 'r', encoding='cp949') as f:\n",
    "            self.news = self.cleaning(f.read())\n",
    "\n",
    "        for _ in self.tokenizer1(self.news):\n",
    "\n",
    "            terms[_] += 1\n",
    "\n",
    "        for _ in self.tokenizer4(self.news):\n",
    "\n",
    "            terms[_] += 1\n",
    "        for _ in self.tokenizer5(self.news):\n",
    "\n",
    "            terms[_] += 1\n",
    "\n",
    "        return terms\n",
    "\n",
    "    def indexer(self, file): # 각 어구, 어절, N-gram 마다 인덱싱\n",
    "\n",
    "        lexicon = defaultdict(int)\n",
    "        for k, v in self.get_tokens(file).items():\n",
    "            lexicon[k] += v\n",
    "\n",
    "        return lexicon\n",
    "\n",
    "    def mergeLexicon(self, Lexicon, i, LocalLexicon, posting):\n",
    "\n",
    "        with open(posting, 'ab') as f:\n",
    "            for k, v in LocalLexicon.items():\n",
    "                termInfo = (i, v, Lexicon[k] if k in Lexicon.keys() else -1)\n",
    "                Lexicon[k] = f.tell()\n",
    "                f.write(pack('iii', termInfo[0], termInfo[1], termInfo[2]))\n",
    "\n",
    "        return Lexicon\n",
    "\n",
    "    def sortedLexicon(self, Lexicon, posting, sposting): # 역 색인 구조\n",
    "\n",
    "        sortedIndex = list()\n",
    "\n",
    "        f1 = open(posting, 'rb')\n",
    "        f2 = open(sposting, 'wb')\n",
    "\n",
    "        for k, v in Lexicon.items():\n",
    "            pos1 = v\n",
    "            pos2 = f2.tell()\n",
    "            df = 0\n",
    "\n",
    "            while pos1 > -1:\n",
    "                f1.seek(pos1)\n",
    "                termInfo = unpack('iii', f1.read(12))\n",
    "                f2.write(pack('ii', termInfo[0], termInfo[1]))\n",
    "                df += 1\n",
    "                pos1 = termInfo[-1]\n",
    "\n",
    "            Lexicon[k] = (df, pos2)\n",
    "        f1.close()\n",
    "        f2.close()\n",
    "        return Lexicon\n",
    "\n",
    "    # TF-IDF를 이용하여 가장 적합한 문서 검색\n",
    "    def search_engine(self, query):\n",
    "\n",
    "        TF = lambda f, mf, a: a + (1 - a)*(f / mf)\n",
    "        IDF = lambda N, df: log(N / df)\n",
    "\n",
    "        FILES = [{'path':_, 'maxfreq':0, 'length':0} for _ in self.fileids()]\n",
    "\n",
    "        N = len(FILES)\n",
    "\n",
    "        posting = 'symptom.dat'\n",
    "        sposting = 'ssymptom.dat'\n",
    "        wposting = 'wsymptom.dat'\n",
    "\n",
    "        globalDictionary = dict()\n",
    "\n",
    "        for docInfo in FILES:\n",
    "            i = FILES.index(docInfo)\n",
    "            localDictionary = self.indexer(docInfo['path'])\n",
    "            FILES[i]['maxfreq'] = max(localDictionary.values())\n",
    "            globalDictionary = self.mergeLexicon(globalDictionary, i, localDictionary, posting)\n",
    "\n",
    "        globalDictionary = self.sortedLexicon(globalDictionary, posting, sposting)\n",
    "\n",
    "        f1 = open(sposting, 'rb')\n",
    "        f2 = open(wposting, 'wb')\n",
    "\n",
    "        for k, v in globalDictionary.items():\n",
    "            docfreq, pos1 = v\n",
    "            f1.seek(pos1)\n",
    "            pos2 = f2.tell()\n",
    "\n",
    "            for _ in range(docfreq):\n",
    "                docid, termfreq = unpack('ii', f1.read(8))\n",
    "                w = TF(termfreq, FILES[docid]['maxfreq'], 0) * IDF(N, docfreq)\n",
    "                f2.write(pack('if', docid, w))\n",
    "                FILES[docid]['length'] += w ** 2\n",
    "\n",
    "            globalDictionary[k] = (docfreq, pos2)\n",
    "\n",
    "        f1.close()\n",
    "        f2.close()\n",
    "\n",
    "\n",
    "        queryDictionary = defaultdict(int)\n",
    "\n",
    "        for _ in self.tokenizer1(query):\n",
    "            queryDictionary[_] += 1\n",
    "\n",
    "        for _ in self.tokenizer4(query):\n",
    "            queryDictionary[_] += 1\n",
    "\n",
    "        for _ in self.tokenizer5(query):\n",
    "            queryDictionary[_] += 1\n",
    "\n",
    "        qmaxfreq = max(queryDictionary.values())\n",
    "        querylength = 0\n",
    "        queryWeight = defaultdict(int)\n",
    "\n",
    "        for k, v in queryDictionary.items():\n",
    "            if k in globalDictionary:\n",
    "                queryWeight[k] = TF(v, qmaxfreq, 0.5) * IDF(N, globalDictionary[k][0])\n",
    "                querylength += queryWeight[k]**2\n",
    "\n",
    "\n",
    "        result = defaultdict(int)\n",
    "\n",
    "        with open(wposting, 'rb') as f:\n",
    "            for k, v in queryWeight.items():\n",
    "                df, pos = globalDictionary[k]\n",
    "                f.seek(pos)\n",
    "                for _ in range(df):\n",
    "                    docid, weight = unpack('if', f.read(8))\n",
    "                    result[docid] += weight*v\n",
    "\n",
    "        for k, v in result.items():\n",
    "            result[k] = v / (sqrt(FILES[k]['length']) * sqrt(querylength))\n",
    "            \n",
    "        with open(\"globalDictionary.json\", \"w\") as json_file:\n",
    "\n",
    "            json.dump(globalDictionary, json_file)\n",
    "            \n",
    "        with open(\"FILES.json\", \"w\") as json_file:\n",
    "\n",
    "            json.dump(FILES, json_file)\n",
    "\n",
    "        K = 3\n",
    "        for _ in list(sorted(result.items(), key=lambda _:-_[1]))[:K]:\n",
    "\n",
    "            with open(FILES[_[0]]['path'], 'r') as f:\n",
    "                return _, f.read().strip()\n",
    "\n",
    "#############################################################################\n",
    "query = '머리아프고, 속이 울렁거린다'\n",
    "\n",
    "\n",
    "a = Search_symptom()\n",
    "b = tokenizer2(query)\n",
    "print(a.search_engine(b[0]))\n",
    "print(a.search_engine(b[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search with existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((9, 0.5421944699333662), '머리가 찌르는 것 같다. 머리아프다. 머리가 극심히 아프다. 머리가 찌르듯 아프다. 머리가 욱신거리다. 머리가 깨질 것 같다. 머리가 터질것 같다. 머리를 싸맨 듯 하다. 머리가 쪼개질 듯 하다. 머리가 타는 듯 하다. 머리를 침으로 찌르는 듯 하다. 머리가 부풀어 깨질 듯 하다. 머리가 무겁고 아프다.', '두통')\n",
      "((5, 0.4279604500510067), '속이 울렁거린다. 메스껍다. 구역감 구역질이 난다. 헛구역질이 난다. 토할 것 같다. 속이 불쾌하다. 오심 메슥거리다.', '근육통')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "import struct\n",
    "from struct import pack, unpack\n",
    "from os import listdir\n",
    "from konlpy.tag import Okt\n",
    "from math import sqrt\n",
    "from math import log\n",
    "import json\n",
    "\n",
    "class Search_symptom:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern1 = re.compile(r'[{}]'.format(re.escape(punctuation)))\n",
    "        self.pattern2 = re.compile(r'\\b(\\w|[.])+@(?:[.]?\\w+)+\\b')\n",
    "        self.pattern3 = re.compile(r'\\bhttps?://\\w+(?:[.]?\\w+)+\\b')\n",
    "        self.pattern4 = re.compile(r'[^A-Za-z0-9가-힣ㄱ-ㅎㅏ-ㅣ ]')\n",
    "        self.pattern5 = re.compile(r'\\b[a-z][A-Za-z0-9]+\\b')\n",
    "        self.pattern6 = re.compile(r'\\s{2,}')\n",
    "        self.pattern7 = re.compile(r'[a-z]')\n",
    "        self.pattern8 = re.compile(r'[/,.]')\n",
    "        self.okt = Okt()\n",
    "\n",
    "    def tokenizer1(self, doc): # 어절\n",
    "\n",
    "        return doc.split()\n",
    "\n",
    "    def tokenizer2(self, doc, n=2): # 어절 Ngram\n",
    "        tmp = doc.split()\n",
    "        ngram = list()\n",
    "        for i in range(0, len(tmp) - n + 1):\n",
    "            token = ''\n",
    "            for j in range(i, i + n):\n",
    "                token += tmp[j] + ' '\n",
    "            ngram.append(token)\n",
    "        return ngram\n",
    "\n",
    "    def tokenizer3(self, doc, n=2): # 음절 Ngram\n",
    "\n",
    "        ngram = list()\n",
    "        for i in range(len(doc) - (n-1)):\n",
    "            ngram.append(doc[i:i+n])\n",
    "        return ngram\n",
    "\n",
    "    def tokenizer4(self, doc): # 형태소\n",
    "\n",
    "        return [_ for _ in self.okt.morphs(doc) if 1 < len(_) < 8]\n",
    "\n",
    "    def tokenizer5(self, doc): # 명사\n",
    "\n",
    "        return [_ for _ in self.okt.nouns(doc) if 1 < len(_) < 8]\n",
    "\n",
    "    def fileids(self, path = './symp/'):\n",
    "        return [path+_ for _ in listdir(path) if re.search('[.]txt$', _)]\n",
    "\n",
    "    def search(self, query):\n",
    "        TF = lambda f, mf, a: a + (1 - a)*(f / mf)\n",
    "        IDF = lambda N, df: log(N / df)\n",
    "\n",
    "        with open('FILES.json','r',encoding = 'utf-8') as f:\n",
    "            FILES = json.load(f)\n",
    "        N = len(FILES)\n",
    "\n",
    "        wposting = 'wsymptom.dat'\n",
    "        with open('globalDictionary.json','r', encoding = 'utf-8') as f:\n",
    "            globalDictionary = json.load(f)\n",
    "\n",
    "        queryDictionary = defaultdict(int)\n",
    "\n",
    "        for _ in self.tokenizer1(query):\n",
    "            queryDictionary[_] += 1\n",
    "        for _ in self.tokenizer4(query):\n",
    "            queryDictionary[_] += 1\n",
    "        for _ in self.tokenizer5(query):\n",
    "            queryDictionary[_] += 1\n",
    "\n",
    "        qmaxfreq = max(queryDictionary.values())\n",
    "        querylength = 0\n",
    "        queryWeight = defaultdict(int)\n",
    "        for k, v in queryDictionary.items():\n",
    "            if k in globalDictionary:\n",
    "                queryWeight[k] = TF(v, qmaxfreq, 0.5) * IDF(N, globalDictionary[k][0])\n",
    "                querylength += queryWeight[k]**2\n",
    "\n",
    "\n",
    "        result = defaultdict(int)\n",
    "\n",
    "        with open(wposting, 'rb') as f:\n",
    "            for k, v in queryWeight.items():\n",
    "                df, pos = globalDictionary[k]\n",
    "                f.seek(pos)\n",
    "                for _ in range(df):\n",
    "                    docid, weight = unpack('if', f.read(8))\n",
    "                    result[docid] += weight*v\n",
    "\n",
    "        for k, v in result.items():\n",
    "            result[k] = v / (sqrt(FILES[k]['length']) * sqrt(querylength))\n",
    "\n",
    "\n",
    "        K = 1\n",
    "        for _ in list(sorted(result.items(), key=lambda _:-_[1]))[:K]:\n",
    "\n",
    "            with open(FILES[_[0]]['path'], 'r') as f:\n",
    "                return _, f.read().strip(), self.pattern7.sub('',self.pattern8.sub('', FILES[_[0]]['path']))\n",
    "\n",
    "######################################################################################################\n",
    "query = '머리아프고, 속이 울렁거린다'\n",
    "\n",
    "\n",
    "a = Search_symptom()\n",
    "b = tokenizer2(query)\n",
    "print(a.search(b[0]))\n",
    "print(a.search(b[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
